{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset:\n",
    "survey_data = pd.read_csv(\"//Users//louis//Downloads//surveydata.csv\")\n",
    "\n",
    "# Q5 was \"How many days per week do you work?\". Converting it as type integer.\n",
    "survey_data[\"Q5\"] = pd.to_numeric(survey_data[\"Q5\"], errors=\"coerce\").fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable types:\n",
    "numerical_vars = [\"Q5\"] # All variables, except for this one, are categorical in our dataset.\n",
    "target_variable = \"Q23\"  # This is the predictor variable.\n",
    "# Q23 was \"Would you be interested in an app that will help you choose more sustainable modes of transport?\".\n",
    "\n",
    "categorical_vars = [col for col in survey_data.columns \n",
    "                    if col not in [target_variable] + numerical_vars]\n",
    "\n",
    "# Remove blanks from the target variable:\n",
    "survey_data = survey_data[survey_data[target_variable] != \" \"]\n",
    "# If we don't remove this, the model will try to predict blank responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Target Variable Encoding: {'1': 0, '2': 1, '3': 2, '4': 3}\n"
     ]
    }
   ],
   "source": [
    "# Encode the target variable (0-Yes, 1-Maybe, 2-No, 3-I don't know):\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(survey_data[target_variable]) # Again, this is our predictor variable (y).\n",
    "\n",
    "# We are applying the encoder to our target variable here. This will result in 4 levels.\n",
    "print(\"New Target Variable Encoding:\", dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable (X):\n",
    "X = survey_data[categorical_vars + numerical_vars]\n",
    "\n",
    "# One-hot encode categorical vars.\n",
    "X = pd.get_dummies(X, columns=categorical_vars, drop_first=True)\n",
    "# drop_first ensures there are no issues of collinearity.\n",
    "\n",
    "# Scale numerical variables:\n",
    "scaler = StandardScaler()\n",
    "X[numerical_vars] = scaler.fit_transform(X[numerical_vars])\n",
    "\n",
    "# Split data for initial Logistic Regression model:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train Logistic Regression Model:\n",
    "log_reg = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    "     solver=\"lbfgs\", # (L-BFGS)\n",
    "      max_iter=500)\n",
    "# Limited-memory Broyden–Fletcher–Goldfarb–Shanno finds the optimal weights for the logistic regression model.\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions:\n",
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.37333333333333335\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.04      0.05        25\n",
      "           1       0.35      0.32      0.33        50\n",
      "           2       0.55      0.64      0.59        58\n",
      "           3       0.11      0.12      0.11        17\n",
      "\n",
      "    accuracy                           0.37       150\n",
      "   macro avg       0.27      0.28      0.27       150\n",
      "weighted avg       0.35      0.37      0.36       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model:\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Feature  Importance\n",
      "45            Q13_6_1    1.194404\n",
      "173             Q26_2    1.082967\n",
      "54            Q14_1_1    1.052972\n",
      "18              Q34_1    0.942319\n",
      "140           Q20_8_1    0.801076\n",
      "164           Q21_7_3    0.755724\n",
      "158           Q21_5_3    0.696857\n",
      "35              Q12_2    0.691313\n",
      "7                Q7_7    0.687125\n",
      "188             Q28_4    0.617354\n",
      "50           Q13_11_1    0.615444\n",
      "118           Q19_5_7    0.593908\n",
      "39              Q12_7    0.585011\n",
      "22            Q10_1_1    0.576920\n",
      "163           Q21_7_1    0.566397\n",
      "88            Q19_1_5    0.554680\n",
      "166           Q21_8_1    0.550014\n",
      "136           Q20_4_1    0.529951\n",
      "10               Q8_1    0.521340\n",
      "24           Q10_12_1    0.505083\n",
      "20              Q34_3    0.503317\n",
      "131           Q19_7_6    0.501199\n",
      "16            Q33_3_1    0.496622\n",
      "72           Q15_11_1    0.448165\n",
      "167           Q21_8_3    0.446660\n",
      "170           Q21_9_3    0.446536\n",
      "59            Q14_7_1    0.442753\n",
      "96            Q19_2_6    0.410790\n",
      "169           Q21_9_1    0.403341\n",
      "178             Q26_7    0.384606\n",
      "120           Q19_6_2    0.379154\n",
      "46            Q13_7_1    0.378291\n",
      "112           Q19_5_1    0.375753\n",
      "77              Q16_3    0.371921\n",
      "196            Q16R_3    0.371921\n",
      "162           Q21_6_5    0.362091\n",
      "56            Q14_4_1    0.360358\n",
      "184             Q25_5    0.341075\n",
      "183             Q25_4    0.338530\n",
      "201     Income_3cat_4    0.338530\n",
      "182             Q25_3    0.334816\n",
      "108           Q19_4_4    0.333263\n",
      "15            Q33_2_1    0.326238\n",
      "91            Q19_2_1    0.324699\n",
      "156           Q21_4_5    0.323961\n",
      "80              Q18_1    0.303716\n",
      "12               Q8_3    0.300460\n",
      "6                Q7_6    0.289966\n",
      "101           Q19_3_4    0.288609\n",
      "121           Q19_6_3    0.287602\n",
      "8                Q7_8    0.272109\n",
      "61            Q14_9_1    0.263429\n",
      "107           Q19_4_3    0.258312\n",
      "147           Q21_1_5    0.251760\n",
      "134           Q20_2_1    0.249857\n",
      "150           Q21_2_5    0.244175\n",
      "81              Q18_2    0.229168\n",
      "137           Q20_5_1    0.223604\n",
      "115           Q19_5_4    0.219846\n",
      "193             Q28_9    0.217524\n",
      "34              Q12_1    0.202097\n",
      "51           Q13_12_1    0.173731\n",
      "79              Q16_5    0.173092\n",
      "194            Q16R_1    0.173092\n",
      "82              Q18_3    0.169074\n",
      "192             Q28_8    0.164394\n",
      "174             Q26_3    0.150092\n",
      "157           Q21_5_1    0.149725\n",
      "40            Q13_1_1    0.147414\n",
      "132           Q19_7_7    0.147142\n",
      "42            Q13_3_1    0.142898\n",
      "128           Q19_7_3    0.142857\n",
      "92            Q19_2_2    0.123537\n",
      "64           Q14_12_1    0.122923\n",
      "126           Q19_7_1    0.113580\n",
      "85            Q19_1_2    0.106873\n",
      "86            Q19_1_3    0.104315\n",
      "133           Q20_1_1    0.098539\n",
      "11               Q8_2    0.097384\n",
      "155           Q21_4_3    0.093818\n",
      "202     Income_3cat_8    0.089799\n",
      "104           Q19_3_7    0.085028\n",
      "153           Q21_3_5    0.082458\n",
      "199  Q7_drive_alone_3    0.082073\n",
      "3                Q7_3    0.082073\n",
      "66            Q15_3_1    0.080647\n",
      "109           Q19_4_5    0.077790\n",
      "135           Q20_3_1    0.076764\n",
      "200     Income_3cat_1    0.072840\n",
      "43            Q13_4_1    0.069235\n",
      "154           Q21_4_1    0.066646\n",
      "65            Q15_2_1    0.065712\n",
      "44            Q13_5_1    0.058757\n",
      "122           Q19_6_4    0.056817\n",
      "26            Q10_4_1    0.045471\n",
      "142          Q20_10_1    0.034826\n",
      "102           Q19_3_5    0.023580\n",
      "161           Q21_6_3    0.010859\n",
      "99            Q19_3_2    0.009157\n",
      "186             Q25_7    0.006958\n",
      "14            Q33_1_1    0.004627\n",
      "144          Q20_12_1    0.004597\n",
      "190             Q28_6    0.000062\n",
      "5                Q7_5   -0.005775\n",
      "57            Q14_5_1   -0.006479\n",
      "4                Q7_4   -0.017579\n",
      "110           Q19_4_6   -0.022086\n",
      "160           Q21_6_1   -0.022904\n",
      "55            Q14_2_1   -0.027770\n",
      "139           Q20_7_1   -0.027773\n",
      "49           Q13_10_1   -0.028608\n",
      "84            Q19_1_1   -0.031005\n",
      "195            Q16R_2   -0.036382\n",
      "78              Q16_4   -0.036382\n",
      "75              Q16_1   -0.052588\n",
      "198            Q16R_5   -0.052588\n",
      "191             Q28_7   -0.055413\n",
      "76              Q16_2   -0.062565\n",
      "197            Q16R_4   -0.062565\n",
      "25            Q10_3_1   -0.065721\n",
      "159           Q21_5_5   -0.067496\n",
      "87            Q19_1_4   -0.070179\n",
      "123           Q19_6_5   -0.076142\n",
      "111           Q19_4_7   -0.078154\n",
      "47            Q13_8_1   -0.085151\n",
      "172             Q22_2   -0.085449\n",
      "151           Q21_3_1   -0.090433\n",
      "185             Q25_6   -0.091092\n",
      "19              Q34_2   -0.096083\n",
      "103           Q19_3_6   -0.097304\n",
      "71            Q15_8_1   -0.097770\n",
      "62           Q14_10_1   -0.120287\n",
      "181             Q25_2   -0.122779\n",
      "149           Q21_2_3   -0.138733\n",
      "180             Q25_1   -0.139197\n",
      "124           Q19_6_6   -0.143019\n",
      "1                Q7_1   -0.148422\n",
      "130           Q19_7_5   -0.159236\n",
      "52           Q13_13_1   -0.166739\n",
      "187             Q25_8   -0.167141\n",
      "176             Q26_5   -0.177723\n",
      "60            Q14_8_1   -0.179638\n",
      "152           Q21_3_3   -0.188701\n",
      "0                  Q5   -0.192967\n",
      "36              Q12_3   -0.197793\n",
      "127           Q19_7_2   -0.206201\n",
      "33           Q10_13_1   -0.214822\n",
      "98            Q19_3_1   -0.228483\n",
      "97            Q19_2_7   -0.228832\n",
      "41            Q13_2_1   -0.228857\n",
      "171           Q21_9_5   -0.229608\n",
      "100           Q19_3_3   -0.230274\n",
      "93            Q19_2_3   -0.237098\n",
      "141           Q20_9_1   -0.238685\n",
      "179             Q26_8   -0.245013\n",
      "175             Q26_4   -0.247331\n",
      "95            Q19_2_5   -0.253770\n",
      "117           Q19_5_6   -0.257995\n",
      "119           Q19_6_1   -0.258418\n",
      "138           Q20_6_1   -0.261163\n",
      "145           Q21_1_1   -0.261692\n",
      "106           Q19_4_2   -0.272999\n",
      "90            Q19_1_7   -0.273097\n",
      "67            Q15_4_1   -0.286704\n",
      "94            Q19_2_4   -0.289014\n",
      "113           Q19_5_2   -0.289209\n",
      "63           Q14_11_1   -0.302368\n",
      "116           Q19_5_5   -0.316589\n",
      "48            Q13_9_1   -0.321150\n",
      "58            Q14_6_1   -0.331583\n",
      "27            Q10_6_1   -0.358297\n",
      "37              Q12_5   -0.360552\n",
      "70            Q15_7_1   -0.371409\n",
      "53           Q13_14_1   -0.378546\n",
      "30            Q10_9_1   -0.382127\n",
      "21              Q34_6   -0.385075\n",
      "146           Q21_1_3   -0.388696\n",
      "125           Q19_6_7   -0.395683\n",
      "189             Q28_5   -0.397092\n",
      "23            Q10_2_1   -0.407926\n",
      "2                Q7_2   -0.424708\n",
      "29            Q10_8_1   -0.443638\n",
      "105           Q19_4_1   -0.445814\n",
      "28            Q10_7_1   -0.452491\n",
      "68            Q15_5_1   -0.469697\n",
      "168           Q21_8_5   -0.473219\n",
      "32           Q10_11_1   -0.475325\n",
      "114           Q19_5_3   -0.475402\n",
      "69            Q15_6_1   -0.482839\n",
      "165           Q21_7_5   -0.495985\n",
      "9                Q7_9   -0.495998\n",
      "83              Q18_4   -0.496548\n",
      "31           Q10_10_1   -0.497577\n",
      "74           Q15_12_1   -0.500892\n",
      "73            Q15_9_1   -0.529726\n",
      "89            Q19_1_6   -0.541273\n",
      "13               Q8_4   -0.634993\n",
      "148           Q21_2_1   -0.650080\n",
      "17            Q33_4_1   -0.680287\n",
      "129           Q19_7_4   -0.689028\n",
      "177             Q26_6   -0.795418\n",
      "38              Q12_6   -0.838003\n",
      "143          Q20_11_1   -0.974492\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance from Logistic Regression:\n",
    "feature_importance = pd.DataFrame(\n",
    "    {\"Feature\": X.columns, \n",
    "      \"Importance\": log_reg.coef_[0]})\n",
    "feature_importance = feature_importance.sort_values(by=\"Importance\", ascending=False)\n",
    "print(feature_importance.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (Positive Importance):\n",
      "['Q13_6_1', 'Q26_2', 'Q14_1_1', 'Q34_1', 'Q20_8_1', 'Q21_7_3', 'Q21_5_3', 'Q12_2', 'Q7_7', 'Q28_4', 'Q13_11_1', 'Q19_5_7', 'Q12_7', 'Q10_1_1', 'Q21_7_1', 'Q19_1_5', 'Q21_8_1', 'Q20_4_1', 'Q8_1', 'Q10_12_1', 'Q34_3', 'Q19_7_6', 'Q33_3_1', 'Q15_11_1', 'Q21_8_3', 'Q21_9_3', 'Q14_7_1', 'Q19_2_6', 'Q21_9_1', 'Q26_7', 'Q19_6_2', 'Q13_7_1', 'Q19_5_1', 'Q16_3', 'Q16R_3', 'Q21_6_5', 'Q14_4_1', 'Q25_5', 'Q25_4', 'Income_3cat_4', 'Q25_3', 'Q19_4_4', 'Q33_2_1', 'Q19_2_1', 'Q21_4_5', 'Q18_1', 'Q8_3', 'Q7_6', 'Q19_3_4', 'Q19_6_3', 'Q7_8', 'Q14_9_1', 'Q19_4_3', 'Q21_1_5', 'Q20_2_1', 'Q21_2_5', 'Q18_2', 'Q20_5_1', 'Q19_5_4', 'Q28_9', 'Q12_1', 'Q13_12_1', 'Q16_5', 'Q16R_1', 'Q18_3', 'Q28_8', 'Q26_3', 'Q21_5_1', 'Q13_1_1', 'Q19_7_7', 'Q13_3_1', 'Q19_7_3', 'Q19_2_2', 'Q14_12_1', 'Q19_7_1', 'Q19_1_2', 'Q19_1_3', 'Q20_1_1', 'Q8_2', 'Q21_4_3', 'Income_3cat_8', 'Q19_3_7', 'Q21_3_5', 'Q7_drive_alone_3', 'Q7_3', 'Q15_3_1', 'Q19_4_5', 'Q20_3_1', 'Income_3cat_1', 'Q13_4_1', 'Q21_4_1', 'Q15_2_1', 'Q13_5_1', 'Q19_6_4', 'Q10_4_1', 'Q20_10_1', 'Q19_3_5', 'Q21_6_3', 'Q19_3_2', 'Q25_7', 'Q33_1_1', 'Q20_12_1', 'Q28_6']\n"
     ]
    }
   ],
   "source": [
    "# Select only variables with positive feature importance.\n",
    "filtered_features = feature_importance[feature_importance[\"Importance\"] > 0]\n",
    "\n",
    "# Extract feature names:\n",
    "new_categorical_vars = filtered_features[\"Feature\"].tolist()\n",
    "print(\"Selected Features (Positive Importance):\")\n",
    "print(new_categorical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-Level Target Encoding: {'1': 0, '2': 1, '3': 2}\n"
     ]
    }
   ],
   "source": [
    "# Create another copy to avoid confusion:\n",
    "survey_data_3level = survey_data.copy()\n",
    "\n",
    "# Convert \"I Don't Know\" into \"Maybe\" to create a 3-Level Target Variable:\n",
    "survey_data_3level[target_variable] = survey_data_3level[target_variable].replace({\"4\": \"2\"})\n",
    "\n",
    "# Encode the target variable for 3-Level Classification:\n",
    "le_3level = LabelEncoder()\n",
    "y_3level = le_3level.fit_transform(survey_data_3level[target_variable])\n",
    "print(\"3-Level Target Encoding:\", dict(zip(le_3level.classes_, le_3level.transform(le_3level.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the important features for training all models:\n",
    "important_features = filtered_features[\"Feature\"].tolist()\n",
    "X_important = X[new_categorical_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-Level Model Accuracy: 0.49333333333333335\n",
      "\n",
      "Classification Report (3-Level):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.20      0.23        25\n",
      "           2       0.51      0.58      0.54        67\n",
      "           3       0.56      0.52      0.54        58\n",
      "\n",
      "    accuracy                           0.49       150\n",
      "   macro avg       0.44      0.43      0.43       150\n",
      "weighted avg       0.48      0.49      0.49       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split and Train 3-Level Logistic Regression Model:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_important, y_3level, test_size=0.2, random_state=42, stratify=y_3level)\n",
    "log_reg = LogisticRegression(\n",
    "    multi_class=\"multinomial\", \n",
    "     solver=\"lbfgs\", \n",
    "      max_iter=500)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate 3-Level Logistic Regression Model:\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(\"3-Level Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report (3-Level):\\n\", classification_report(y_test, y_pred, target_names=le_3level.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.5\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        25\n",
      "           1       0.49      0.66      0.56        67\n",
      "           2       0.53      0.53      0.53        58\n",
      "\n",
      "    accuracy                           0.50       150\n",
      "   macro avg       0.34      0.40      0.36       150\n",
      "weighted avg       0.43      0.50      0.46       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest Model on All Features:\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate Random Forest Model:\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Feature Importance:\n",
      "               Feature  Importance\n",
      "97            Q21_6_3    0.026544\n",
      "87            Q20_3_1    0.021471\n",
      "4             Q20_8_1    0.020270\n",
      "45              Q18_1    0.020060\n",
      "5             Q21_7_3    0.019633\n",
      "25            Q21_9_3    0.019270\n",
      "66              Q26_3    0.019267\n",
      "98            Q19_3_2    0.019081\n",
      "37              Q25_5    0.018262\n",
      "56              Q18_2    0.018227\n",
      "79            Q21_4_3    0.018036\n",
      "58            Q19_5_4    0.017785\n",
      "90            Q21_4_1    0.017633\n",
      "77            Q20_1_1    0.017462\n",
      "67            Q21_5_1    0.017449\n",
      "54            Q20_2_1    0.017291\n",
      "7               Q12_2    0.017049\n",
      "19           Q10_12_1    0.017007\n",
      "6             Q21_5_3    0.016988\n",
      "14            Q21_7_1    0.016703\n",
      "27            Q19_2_6    0.016530\n",
      "95           Q20_10_1    0.015634\n",
      "41            Q19_4_4    0.015107\n",
      "28            Q21_9_1    0.014444\n",
      "52            Q19_4_3    0.014224\n",
      "48            Q19_3_4    0.014027\n",
      "75            Q19_1_2    0.013885\n",
      "13            Q10_1_1    0.013683\n",
      "33              Q16_3    0.013240\n",
      "80      Income_3cat_8    0.012949\n",
      "84               Q7_3    0.012927\n",
      "65              Q28_8    0.012602\n",
      "69            Q19_7_7    0.012585\n",
      "12              Q12_7    0.012399\n",
      "39      Income_3cat_4    0.012358\n",
      "93            Q19_6_4    0.012254\n",
      "38              Q25_4    0.012066\n",
      "34             Q16R_3    0.011988\n",
      "86            Q19_4_5    0.011899\n",
      "83   Q7_drive_alone_3    0.011593\n",
      "94            Q10_4_1    0.011536\n",
      "62              Q16_5    0.011295\n",
      "17            Q20_4_1    0.011240\n",
      "24            Q21_8_3    0.011123\n",
      "32            Q19_5_1    0.011039\n",
      "99              Q25_7    0.010872\n",
      "72            Q19_2_2    0.010812\n",
      "101          Q20_12_1    0.010783\n",
      "63             Q16R_1    0.010125\n",
      "60              Q12_1    0.009799\n",
      "76            Q19_1_3    0.009539\n",
      "59              Q28_9    0.009488\n",
      "49            Q19_6_3    0.009331\n",
      "16            Q21_8_1    0.009175\n",
      "96            Q19_3_5    0.008876\n",
      "91            Q15_2_1    0.008839\n",
      "85            Q15_3_1    0.008403\n",
      "51            Q14_9_1    0.008195\n",
      "88      Income_3cat_1    0.008170\n",
      "57            Q20_5_1    0.007724\n",
      "44            Q21_4_5    0.007681\n",
      "30            Q19_6_2    0.007243\n",
      "89            Q13_4_1    0.006896\n",
      "10           Q13_11_1    0.006830\n",
      "35            Q21_6_5    0.006621\n",
      "1               Q26_2    0.006574\n",
      "8                Q7_7    0.006287\n",
      "100           Q33_1_1    0.005878\n",
      "22            Q33_3_1    0.005626\n",
      "82            Q21_3_5    0.005537\n",
      "42            Q33_2_1    0.005469\n",
      "92            Q13_5_1    0.005425\n",
      "40              Q25_3    0.005380\n",
      "64              Q18_3    0.005348\n",
      "74            Q19_7_1    0.005103\n",
      "2             Q14_1_1    0.004914\n",
      "55            Q21_2_5    0.004860\n",
      "29              Q26_7    0.004684\n",
      "36            Q14_4_1    0.004579\n",
      "15            Q19_1_5    0.004563\n",
      "21            Q19_7_6    0.004483\n",
      "61           Q13_12_1    0.004258\n",
      "0             Q13_6_1    0.003801\n",
      "11            Q19_5_7    0.003284\n",
      "43            Q19_2_1    0.003193\n",
      "23           Q15_11_1    0.002585\n",
      "78               Q8_2    0.002408\n",
      "46               Q8_3    0.002363\n",
      "71            Q19_7_3    0.002256\n",
      "31            Q13_7_1    0.002195\n",
      "3               Q34_1    0.001990\n",
      "73           Q14_12_1    0.001895\n",
      "53            Q21_1_5    0.001750\n",
      "9               Q28_4    0.001418\n",
      "68            Q13_1_1    0.001362\n",
      "102             Q28_6    0.001280\n",
      "26            Q14_7_1    0.001234\n",
      "18               Q8_1    0.001038\n",
      "47               Q7_6    0.000920\n",
      "70            Q13_3_1    0.000829\n",
      "20              Q34_3    0.000763\n",
      "50               Q7_8    0.000542\n",
      "81            Q19_3_7    0.000410\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance from Random Forest:\n",
    "feature_importance_rf = pd.DataFrame(\n",
    "    {\"Feature\": X_important.columns,\n",
    "      \"Importance\": rf_model.feature_importances_})\n",
    "feature_importance_rf = feature_importance_rf.sort_values(by=\"Importance\", ascending=False)\n",
    "print(\"\\nRandom Forest Feature Importance:\\n\", feature_importance_rf.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy (Important Features Only): 0.5\n",
      "\n",
      "Random Forest Classification Report (Important Features Only):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        25\n",
      "           2       0.49      0.66      0.56        67\n",
      "           3       0.53      0.53      0.53        58\n",
      "\n",
      "    accuracy                           0.50       150\n",
      "   macro avg       0.34      0.40      0.36       150\n",
      "weighted avg       0.43      0.50      0.46       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Model on Important Features Only:\n",
    "X_train_rf_imp, X_test_rf_imp, y_train_rf_imp, y_test_rf_imp = train_test_split(X_important, y_3level, test_size=0.2, random_state=42, stratify=y_3level)\n",
    "rf_model_imp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model_imp.fit(X_train_rf_imp, y_train_rf_imp)\n",
    "y_pred_rf_imp = rf_model_imp.predict(X_test_rf_imp)\n",
    "\n",
    "# Evaluate Performance\n",
    "print(\"Random Forest Accuracy (Important Features Only):\", accuracy_score(y_test_rf_imp, y_pred_rf_imp))\n",
    "print(\"\\nRandom Forest Classification Report (Important Features Only):\\n\", classification_report(y_test_rf_imp, y_pred_rf_imp, target_names=le_3level.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.4266666666666667\n",
      "\n",
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.12      0.17        25\n",
      "           2       0.43      0.49      0.46        67\n",
      "           3       0.44      0.48      0.46        58\n",
      "\n",
      "    accuracy                           0.43       150\n",
      "   macro avg       0.38      0.37      0.36       150\n",
      "weighted avg       0.41      0.43      0.41       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost Model on All Features:\n",
    "xgb_model = XGBClassifier(eval_metric=\"mlogloss\", random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate XGBoost Model:\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"\\nXGBoost Classification Report:\\n\", classification_report(y_test, y_pred_xgb, target_names=le_3level.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy (Important Features Only): 0.4266666666666667\n",
      "\n",
      "XGBoost Classification Report (Important Features Only):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.12      0.17        25\n",
      "           2       0.43      0.49      0.46        67\n",
      "           3       0.44      0.48      0.46        58\n",
      "\n",
      "    accuracy                           0.43       150\n",
      "   macro avg       0.38      0.37      0.36       150\n",
      "weighted avg       0.41      0.43      0.41       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost Model on Important Features Only:\n",
    "X_train_xgb_imp, X_test_xgb_imp, y_train_xgb_imp, y_test_xgb_imp = train_test_split(X_important, y_3level, test_size=0.2, random_state=42, stratify=y_3level)\n",
    "xgb_model_imp = XGBClassifier(eval_metric=\"mlogloss\", random_state=42)\n",
    "xgb_model_imp.fit(X_train_xgb_imp, y_train_xgb_imp)\n",
    "y_pred_xgb_imp = xgb_model_imp.predict(X_test_xgb_imp)\n",
    "\n",
    "# Evaluate Performance:\n",
    "print(\"XGBoost Accuracy (Important Features Only):\", accuracy_score(y_test_xgb_imp, y_pred_xgb_imp))\n",
    "print(\"\\nXGBoost Classification Report (Important Features Only):\\n\", classification_report(y_test_xgb_imp, y_pred_xgb_imp, target_names=le_3level.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Feature Importance After Retraining:\n",
      "               Feature  Importance\n",
      "2             Q14_1_1    0.026892\n",
      "25            Q21_9_3    0.026757\n",
      "3               Q34_1    0.025304\n",
      "42            Q33_2_1    0.023985\n",
      "43            Q19_2_1    0.022210\n",
      "64              Q18_3    0.020229\n",
      "17            Q20_4_1    0.019760\n",
      "6             Q21_5_3    0.017528\n",
      "97            Q21_6_3    0.016573\n",
      "57            Q20_5_1    0.016541\n",
      "89            Q13_4_1    0.016047\n",
      "101          Q20_12_1    0.015671\n",
      "62              Q16_5    0.014186\n",
      "44            Q21_4_5    0.014017\n",
      "82            Q21_3_5    0.013871\n",
      "49            Q19_6_3    0.013581\n",
      "13            Q10_1_1    0.013512\n",
      "31            Q13_7_1    0.013419\n",
      "8                Q7_7    0.013239\n",
      "32            Q19_5_1    0.013161\n",
      "96            Q19_3_5    0.012851\n",
      "88      Income_3cat_1    0.012768\n",
      "85            Q15_3_1    0.012672\n",
      "73           Q14_12_1    0.012467\n",
      "4             Q20_8_1    0.012375\n",
      "67            Q21_5_1    0.011981\n",
      "1               Q26_2    0.011938\n",
      "33              Q16_3    0.011788\n",
      "35            Q21_6_5    0.011323\n",
      "14            Q21_7_1    0.011207\n",
      "29              Q26_7    0.011168\n",
      "93            Q19_6_4    0.011045\n",
      "37              Q25_5    0.011031\n",
      "55            Q21_2_5    0.010910\n",
      "36            Q14_4_1    0.010813\n",
      "76            Q19_1_3    0.010791\n",
      "10           Q13_11_1    0.010790\n",
      "7               Q12_2    0.010787\n",
      "60              Q12_1    0.010509\n",
      "99              Q25_7    0.010445\n",
      "5             Q21_7_3    0.010420\n",
      "15            Q19_1_5    0.010291\n",
      "22            Q33_3_1    0.010290\n",
      "30            Q19_6_2    0.010211\n",
      "38              Q25_4    0.010196\n",
      "86            Q19_4_5    0.010167\n",
      "21            Q19_7_6    0.010091\n",
      "56              Q18_2    0.010068\n",
      "59              Q28_9    0.010028\n",
      "61           Q13_12_1    0.010022\n",
      "69            Q19_7_7    0.009857\n",
      "12              Q12_7    0.009837\n",
      "9               Q28_4    0.009780\n",
      "58            Q19_5_4    0.009762\n",
      "98            Q19_3_2    0.009696\n",
      "24            Q21_8_3    0.009631\n",
      "0             Q13_6_1    0.009605\n",
      "75            Q19_1_2    0.009534\n",
      "66              Q26_3    0.009354\n",
      "45              Q18_1    0.009317\n",
      "27            Q19_2_6    0.009276\n",
      "19           Q10_12_1    0.009268\n",
      "79            Q21_4_3    0.009157\n",
      "52            Q19_4_3    0.008924\n",
      "92            Q13_5_1    0.008795\n",
      "95           Q20_10_1    0.008793\n",
      "91            Q15_2_1    0.008690\n",
      "77            Q20_1_1    0.008569\n",
      "87            Q20_3_1    0.008557\n",
      "65              Q28_8    0.008544\n",
      "54            Q20_2_1    0.008533\n",
      "72            Q19_2_2    0.008409\n",
      "40              Q25_3    0.008380\n",
      "90            Q21_4_1    0.008371\n",
      "16            Q21_8_1    0.008324\n",
      "83   Q7_drive_alone_3    0.008081\n",
      "94            Q10_4_1    0.007810\n",
      "51            Q14_9_1    0.007781\n",
      "80      Income_3cat_8    0.007276\n",
      "11            Q19_5_7    0.007079\n",
      "41            Q19_4_4    0.006852\n",
      "100           Q33_1_1    0.006843\n",
      "48            Q19_3_4    0.006588\n",
      "28            Q21_9_1    0.005639\n",
      "71            Q19_7_3    0.005515\n",
      "74            Q19_7_1    0.005370\n",
      "23           Q15_11_1    0.004779\n",
      "46               Q8_3    0.002990\n",
      "78               Q8_2    0.002511\n",
      "34             Q16R_3    0.000000\n",
      "18               Q8_1    0.000000\n",
      "20              Q34_3    0.000000\n",
      "26            Q14_7_1    0.000000\n",
      "47               Q7_6    0.000000\n",
      "39      Income_3cat_4    0.000000\n",
      "50               Q7_8    0.000000\n",
      "53            Q21_1_5    0.000000\n",
      "63             Q16R_1    0.000000\n",
      "68            Q13_1_1    0.000000\n",
      "84               Q7_3    0.000000\n",
      "70            Q13_3_1    0.000000\n",
      "81            Q19_3_7    0.000000\n",
      "102             Q28_6    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance for Updated XGBoost Model:\n",
    "feature_importance_xgb_imp = pd.DataFrame(\n",
    "    {\"Feature\": X_important.columns,\n",
    "      \"Importance\": xgb_model_imp.feature_importances_})\n",
    "feature_importance_xgb_imp = feature_importance_xgb_imp.sort_values(by=\"Importance\", ascending=False)\n",
    "print(\"\\nXGBoost Feature Importance After Retraining:\\n\", feature_importance_xgb_imp.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
